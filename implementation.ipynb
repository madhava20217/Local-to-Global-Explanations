{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: ucimlrepo in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: lime in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from lime) (3.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from lime) (4.65.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from lime) (0.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (10.0.1)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (2.31.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (2023.8.30)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from scikit-image>=0.12->lime) (0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from matplotlib->lime) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from matplotlib->lime) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from matplotlib->lime) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\torchnew\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping c:\\ProgramData\\Anaconda3\\envs\\torchnew\\Lib\\site-packages\\numpy-1.26.0.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn ucimlrepo lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from scipy.special import softmax\n",
    "import shap\n",
    "\n",
    "MODEL_NAME = 'Wine'\n",
    "\n",
    "MODEL_FUNCTION = DecisionTreeClassifier\n",
    "model_params = {}\n",
    "coding = {\"Wine\": 109}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "split_size = 0.3\n",
    "seed = 1234567\n",
    "\n",
    "dataset_id = coding[MODEL_NAME]\n",
    "\n",
    "wine = fetch_ucirepo(id=109) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes) \n",
    "X = wine.data.features\n",
    "y = wine.data.targets.to_numpy()\n",
    "y = y.reshape((len(y), ))\n",
    "    \n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "n_feats = X.shape[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = split_size, random_state = seed)\n",
    "\n",
    "normalizer = Normalizer().fit(x_train)\n",
    "encoder = LabelEncoder().fit(y_train)\n",
    "\n",
    "x_train = normalizer.transform(x_train)\n",
    "x_test = normalizer.transform(x_test)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test  = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, samples, targets):\n",
    "    pred = model.predict(samples)\n",
    "    return accuracy_score(targets, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "model = MODEL_FUNCTION(*model_params)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "acc_train = evaluate_model(model, x_train, y_train)\n",
    "acc_test  = evaluate_model(model, x_test, y_test)\n",
    "\n",
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{MODEL_NAME}.pkl', 'wb') as f:\n",
    "    pkl.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Local2GlobalExplainer:\n",
    "    def __init__(self, x_train, model):\n",
    "        self.model = model\n",
    "        self.data = x_train\n",
    "        \n",
    "        self.explainer = lime_tabular.LimeTabularExplainer(\n",
    "            training_data=self.data, \n",
    "            mode = 'classification'\n",
    "            )\n",
    "        \n",
    "    def get_optimal_gmm(n_components):\n",
    "        c = round(n_components)\n",
    "        gmm = GaussianMixture(n_components = c).fit(x_train)\n",
    "        return gmm.bic(x_train)\n",
    "        \n",
    "    def get_local_interpretation(self, sample, num_features):\n",
    "    \n",
    "        exp = self.explainer.explain_instance(sample, model.predict, num_features = len(sample))\n",
    "        local_exp = list(exp.local_exp.values())[0]\n",
    "        local_exp = sorted(local_exp)\n",
    "        \n",
    "        explanations = [x[1] for x in local_exp]\n",
    "        return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_interpretation(explainer, predict_function, sample, num_features):\n",
    "    \n",
    "    exp = explainer.explain_instance(sample, predict_function, num_features = len(sample))\n",
    "    local_exp = list(exp.local_exp.values())[0]\n",
    "    local_exp = sorted(local_exp)\n",
    "    \n",
    "    explanations = [x[1] for x in local_exp]\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n"
     ]
    }
   ],
   "source": [
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data = x_train,\n",
    "    mode = 'classification'\n",
    ")\n",
    "\n",
    "def get_optimal_gmm(n_components):\n",
    "    c = round(n_components)\n",
    "    gmm = GaussianMixture(n_components = c).fit(x_train)\n",
    "    return gmm.bic(x_train)\n",
    "\n",
    "parameters = {'n_components': (0, len(x_train)//4)}\n",
    "\n",
    "gmm = BayesianGaussianMixture(n_components= n_classes*2).fit(x_train)\n",
    "# gbm_bo = BayesianOptimization(get_optimal_gmm, parameters, random_state=111, allow_duplicate_points=True)\n",
    "# gbm_bo.maximize(init_points = 30, n_iter = 10)\n",
    "# params_gbm = gbm_bo.max['params']\n",
    "# n_comp = round(params_gbm['n_components'])\n",
    "\n",
    "# gmm = GaussianMixture(n_components= n_comp).fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(ex):\n",
    "    '''Function to normalize scores using a softmax function multiplied by correlation signs'''\n",
    "    signs = np.sign(ex)\n",
    "    abs_softmax = softmax(np.abs(ex))\n",
    "    return abs_softmax*signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explainability(fit_dist, train_samples, model, explainer, main_dist, num_samples, mcmc = False):\n",
    "    if mcmc == True:\n",
    "        # run monte carlo trials\n",
    "        samples, gmm_class = fit_dist.sample(num_samples)       # generate samples from the fit gmm\n",
    "        explanations = []                                       # list to store explanations\n",
    "        for sample in tqdm(samples):\n",
    "            interpret = get_local_interpretation(explainer, model.predict_proba, sample, n_feats)\n",
    "            sigmoid_interpretation = get_scores(interpret)\n",
    "            explanations.append(sigmoid_interpretation)\n",
    "            \n",
    "        agg_explanations = np.mean(np.array(explanations), axis = 0)        # aggregating\n",
    "        \n",
    "        return agg_explanations, explanations\n",
    "    \n",
    "\n",
    "    else:\n",
    "        # importance sampling\n",
    "        for i in train_samples:\n",
    "            pass\n",
    "    \n",
    "        \n",
    "        \n",
    "def rank_explanations(explanations):\n",
    "    return sorted(list(zip(range(len(explanations)), explanations)), key = lambda x: -abs(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 64.15it/s]\n"
     ]
    }
   ],
   "source": [
    "aggregated, point_explanations = run_explainability(gmm, x_train, model, explainer, None, 1000, mcmc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, -0.04427801177176151),\n",
       " (0, 0.010350171452497546),\n",
       " (3, 0.0076402752498283535),\n",
       " (10, 0.006595678668763183),\n",
       " (12, -0.0036595638779221337),\n",
       " (1, -0.0030315274890849358),\n",
       " (5, -0.0009369094913669249),\n",
       " (4, 0.0009242944614868953),\n",
       " (7, -0.0008775219894062209),\n",
       " (2, -0.0008659531302355162),\n",
       " (8, -0.0005055154321299345),\n",
       " (11, 0.0003654287180868321),\n",
       " (6, -0.00016309288481481266)]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_explanations(aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_feature_importances(model, x_train):\n",
    "    se = shap.Explainer(model)\n",
    "    shap_values = se.shap_values(x_train)\n",
    "    importance_order = np.argsort(-abs(np.abs(np.array(shap_values)).mean(axis = 1).mean(axis = 0)))\n",
    "    importances = np.abs(np.array(shap_values)).mean(axis = 1).mean(axis = 0)[importance_order]\n",
    "    return importance_order, importances\n",
    "\n",
    "order, importances = get_shap_feature_importances(model, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local2Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "vals_ = list(set(np.arange(n_feats)).difference(set([9,0,3,10])))\n",
    "model_l2g = MODEL_FUNCTION(*model_params)\n",
    "model_l2g.fit(x_train[:, vals], y_train)\n",
    "\n",
    "acc_train_l2g = evaluate_model(model_l2g, x_train[:, vals], y_train)\n",
    "acc_test_l2g  = evaluate_model(model_l2g, x_test[:, vals], y_test)\n",
    "\n",
    "print(acc_train_l2g, acc_test_l2g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap features are 3, 6, 10, 9\n",
    "vals = list(set(np.arange(n_feats)).difference(set([3,6,10,9])))\n",
    "\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8703703703703703\n"
     ]
    }
   ],
   "source": [
    "model_shap = MODEL_FUNCTION(*model_params)\n",
    "model_shap.fit(x_train[:, vals], y_train)\n",
    "\n",
    "acc_train_shap = evaluate_model(model_shap, x_train[:, vals], y_train)\n",
    "acc_test_shap  = evaluate_model(model_shap, x_test[:, vals], y_test)\n",
    "\n",
    "print(acc_train_shap, acc_test_shap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('torchnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae2c75324e7fdf5ebd22146e2daffaa477f8ea149f0e685be4c317c2939a685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
